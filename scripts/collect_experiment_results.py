# Parses the log files of an experiment
# Loads checkpoints of all kinds and aggregates data on their performance
# and the counterexample violation
# Also aggregates information across experiment cases in one experiment
from typing import Dict, Tuple, List, Optional, Set, Any, Sequence
import typing
from warnings import warn

from enum import Enum, auto
from dataclasses import dataclass
from collections import defaultdict, OrderedDict
import re

import argparse
from pathlib import Path
import os
from tempfile import TemporaryFile, TemporaryDirectory

import dill
import ruamel.yaml
from natsort import natsorted

import torch
from torch.utils.data import DataLoader
import numpy as np
import pandas
from math import prod

import itertools
import multiprocessing as mp
from copy import deepcopy
from tqdm import tqdm

from deep_opt import Property, NeuralNetwork, RobustnessPropertyFactory
from experiments.datasets import integer_dataset
from nn_repair.datasets import Adult
from nn_repair.falsifiers import ProjectedGradientDescentAttack
from nn_repair.verifiers import ERAN
from nn_repair.datasets.quasirandom import SobolDataset
from nn_repair.utils.random_sampling import sample_from_normal_in_range

from experiments import datasets
from experiments.experiment_base import QUADRATIC_PENALTY_KEY, L1_PENALTY_KEY, L1_PLUS_QUADRATIC_PENALTY_KEY, \
    AUGMENTED_LAGRANGIAN_KEY, LN_BARRIER_KEY, RECIPROCAL_BARRIER_KEY, L1_PENALTY_RECIPROCAL_BARRIER_KEY, \
    DATASET_AUGMENTATION_KEY, FINE_TUNING_KEY, LP_MINIMAL_MODIFICATION_KEY, NEURON_FIXATION_KEY

from scripts.calculate_network_accuracies import full_accuracy
from scripts.calculate_local_robustness_bounds import check_local_robustness


class ExperimentResult(Enum):
    SUCCESS = auto()
    BACKEND_FAILURE = auto()
    BACKEND_ERROR = auto()  # currently not in use
    MAX_ITERATIONS_FAILURE = auto()
    VERIFICATION_PROBLEM = auto()
    TIMEOUT = auto()
    ABORTED = auto()

    def __str__(self):
        names = {
            ExperimentResult.SUCCESS: 'success',
            ExperimentResult.BACKEND_FAILURE: 'backend_failure',
            ExperimentResult.BACKEND_ERROR: 'backend_error',
            ExperimentResult.MAX_ITERATIONS_FAILURE: 'max_iterations_failure',
            ExperimentResult.VERIFICATION_PROBLEM: 'verification_problem',
            ExperimentResult.TIMEOUT: 'timeout',
            ExperimentResult.ABORTED: 'aborted',
        }
        return names[self]


class RepairBackend(Enum):
    QUADRATIC_PENALTY = auto()
    L1_PENALTY = auto()
    L1_PLUS_QUADRATIC_PENALTY = auto()
    AUGMENTED_LAGRANGIAN = auto()
    LN_BARRIER = auto()
    RECIPROCAL_BARRIER = auto()
    L1_PENALTY_RECIPROCAL_BARRIER = auto()
    DATASET_AUGMENTATION = auto()
    FINE_TUNING = auto()
    LP_MINIMAL_MODIFICATION = auto()
    NEURON_FIXATION = auto()

    def __str__(self):
        names = {
            RepairBackend.QUADRATIC_PENALTY: 'quadratic penalty function',
            RepairBackend.L1_PENALTY: 'l1 penalty function',
            RepairBackend.L1_PLUS_QUADRATIC_PENALTY: 'l1 plus quadratic penalty function',
            RepairBackend.AUGMENTED_LAGRANGIAN: 'augmented lagrangian',
            RepairBackend.LN_BARRIER: 'ln barrier',
            RepairBackend.RECIPROCAL_BARRIER: 'reciprocal barrier',
            RepairBackend.L1_PENALTY_RECIPROCAL_BARRIER: 'l1 penalty and reciprocal barrier',
            RepairBackend.DATASET_AUGMENTATION: 'dataset augmentation',
            RepairBackend.FINE_TUNING: 'fine tuning',
            RepairBackend.LP_MINIMAL_MODIFICATION: 'linear programming minimal modification',
            RepairBackend.NEURON_FIXATION: 'neuron fixation'
        }
        return names[self]


class LogFileParser:
    """
    Rudimentary parser for extracting information from a log file generated by running an
    ExperimentBase-based experiment (case) or similarly structured experiments.
    """
    float_value_rx = re.compile(r'(?P<value>-?\d+(\.\d+)?)')
    bracket_name_rx = re.compile(r'\((?P<name>[^)]*)\)')
    colon_delimited_name_rx = re.compile(r'(?P<name>[^:]+):')
    network_name_rx = re.compile(r' *(network|architecture|rmi): (?P<name>.*)')
    verifier_rx = re.compile(r' *verifier: (?P<name>.*)')
    falsifiers_rx = re.compile(r' *falsifiers: (?P<name>.*)')
    verifier_exit_mode_rx = re.compile(r' *verifier_exit_mode: (?P<name>.*)')
    repair_level_loss_rx = re.compile(r'(?P<name>[^ ]+) loss: +(?P<values>(-?[0-9]+\.[0-9]+ \(.*\);?)+)')
    counterexample_art_rx = re.compile(r' *│ (?P<violations>[^ ]+) +│')
    training_loss_rx = re.compile(r'iteration: (?P<iteration>[0-9]+) \| (?P<values>([^:]+: -?[0-9]+\.[0-9]+,?)+)')
    epoch_training_loss_rx = re.compile(r'epoch: (?P<epoch>[0-9]+) \(iteration: (?P<epoch_iteration>[0-9]+), *[0-9]+%\)'
                                        r' \| (?P<values>([^:]+: -?[0-9]+\.[0-9]+,?)+)')
    repair_network_iteration_rx = re.compile(r'repair_network iteration (?P<repair_step>[0-9]+)')
    # always check total_runtime_rx before runtime_rx
    total_runtime_rx = re.compile(r"Executing overall repair took: (?P<time>[0-9]+\.[0-9]+) seconds\.")
    runtime_rx = re.compile(r'Executing (?P<name>.*) took: (?P<time>[0-9]+\.[0-9]+) seconds\.')
    repair_successful_rx = re.compile(r'(All properties verified! Repair successful\.)'
                                      r'|(Repair finished: success)')
    backend_failure_rx = re.compile(r'(Backend could not repair counterexamples. Aborting repair\.)'
                                    r'|(Repair finished: failure)')
    max_iterations_failure_rx = re.compile(r'Repair failed: Maximum number of iterations exhausted\.')
    verification_problem_rx = re.compile(r'The following properties could not be verified due to errors: '
                                         r'\[(?P<properties>.*)]')
    timeout_rx = re.compile(r'Experiment timed out')
    modified_network_saved = re.compile(r'Saving repaired network in file: .*')

    repair_backend_rxs = {
        RepairBackend.QUADRATIC_PENALTY: re.compile(QUADRATIC_PENALTY_KEY + r': True'),
        RepairBackend.L1_PENALTY: re.compile(L1_PENALTY_KEY + r': True'),
        RepairBackend.L1_PLUS_QUADRATIC_PENALTY: re.compile(L1_PLUS_QUADRATIC_PENALTY_KEY + r': True'),
        RepairBackend.AUGMENTED_LAGRANGIAN: re.compile(AUGMENTED_LAGRANGIAN_KEY + r': True'),
        RepairBackend.LN_BARRIER: re.compile(LN_BARRIER_KEY + r': True'),
        RepairBackend.RECIPROCAL_BARRIER: re.compile(RECIPROCAL_BARRIER_KEY + r': True'),
        RepairBackend.L1_PENALTY_RECIPROCAL_BARRIER: re.compile(L1_PENALTY_RECIPROCAL_BARRIER_KEY + r': True'),
        RepairBackend.DATASET_AUGMENTATION: re.compile(DATASET_AUGMENTATION_KEY + r': True'),
        RepairBackend.FINE_TUNING: re.compile(FINE_TUNING_KEY + r': True'),
        RepairBackend.LP_MINIMAL_MODIFICATION: re.compile(LP_MINIMAL_MODIFICATION_KEY + r': True'),
        RepairBackend.NEURON_FIXATION: re.compile(NEURON_FIXATION_KEY + r': True'),
    }

    class LogParseError(RuntimeError):
        pass

    def __init__(self, epoch_length_):
        self.repair_backend = None
        self.repair_step = 0
        self.initial_loss: Optional[Dict[str, float]] = None
        self.final_loss: Optional[Dict[str, float]] = None
        self.network_name = None
        self.verifier = None
        self.falsifiers = None
        self.verifier_exit_mode = None
        self.backend_iterations_of_repair_step: List[int] = []  # also used to count for the current backend run
        # key: (repair_step, backend_iteration)
        self.counterexamples_violated: Dict[Tuple[int, int], int] = {}
        # key: (repair_step, backend_iteration, training_iteration)
        self.training_losses: Dict[Tuple[int, int, int], Dict[str, float]] = defaultdict(dict)
        # key: (repair_step, backend_iteration)
        self.training_iterations_of_run: Dict[Tuple[int, int], int] = {}
        self.training_loop_run = 0
        self.training_loop_runs_of_repair_step: List[List[int]] = []
        self.training_loop_runs_to_backend_iteration: List[int] = []
        # key: tool, value: repair step to runtime, values should be OrderedDict
        self.runtimes: Dict[str, Dict[int, float]] = defaultdict(OrderedDict)
        self.total_runtime: Optional[float] = None
        self.experiment_result = None
        self.not_verified_properties = None

        self.epoch_length = epoch_length_

        self.line = None  # for C-style file reading (readline() method)
        self.match = None  # makes the regex match accessible outside the matches() method
        self.epoch_losses = False  # used by the matches_training_loss_log() method

    def readline(self, file):
        self.line = file.readline()
        return len(self.line) > 0

    def matches(self, pattern, string):
        self.match = pattern.match(string)
        return self.match

    def contains(self, pattern, string):
        self.match = pattern.search(string)
        return self.match

    def matches_training_loss_log(self, string):
        if self.matches(self.training_loss_rx, string):
            self.epoch_losses = False
            return self.match
        elif self.matches(self.epoch_training_loss_rx, string):
            assert self.epoch_length is not None, \
                'Log file contains training losses logged in epochs, but epoch length is unknown.'
            self.epoch_losses = True
            return self.match
        else:
            return None

    def loss_string_to_dict(self, loss_string) -> Dict[str, float]:
        # format: 123.4567 (name); 42.7712 (name2);
        return dict(
            (
                self.bracket_name_rx.search(val_str).group('name'),
                float(self.float_value_rx.match(val_str).group('value'))  # is at start => match works fine
            )
            for val_str in map(str.strip, loss_string.split(';')) if val_str != ''
        )

    # unfortunately the code does not consistently log losses
    def loss_string_to_dict_training(self, loss_string) -> Dict[str, float]:
        # format: name: 123.4567, name2: 42.7712
        return dict(
            (
                self.colon_delimited_name_rx.match(val_str).group('name'),
                float(self.float_value_rx.search(val_str).group('value'))
            )
            for val_str in map(str.strip, loss_string.split(',')) if val_str != ''
        )

    def read_away_counterexample_art(self, log_file):
        # concatenate the violation symbols on all lines of the counterexample art
        violation_symbols = ''
        while self.matches(self.counterexample_art_rx, self.line):
            violation_symbols += self.match.group('violations')
            if not self.readline(log_file):
                break
        backend_iteration_ = self.backend_iterations_of_repair_step[-1]
        violated = [symbol != '0' for symbol in violation_symbols]
        self.counterexamples_violated[(self.repair_step, backend_iteration_)] = sum(violated)

    def read_away_training_loop(self, log_file):
        """
        Reads the output of a training loop until no logged loss lines follow.
        """
        # first line was already read
        training_loss_line_present = True
        training_iteration_ = 0
        while training_loss_line_present:
            if self.epoch_losses:
                training_epoch_ = int(self.match.group('epoch'))
                epoch_iteration_ = int(self.match.group('epoch_iteration'))
                training_iteration_ = training_epoch_ * self.epoch_length + epoch_iteration_
            else:
                training_iteration_ = int(self.match.group('iteration'))
            loss_key = (self.repair_step, self.backend_iterations_of_repair_step[-1], training_iteration_)
            self.training_losses[loss_key] = self.loss_string_to_dict_training(self.match.group('values'))

            if not self.readline(log_file):
                return  # this means that the experiment was aborted
            training_loss_line_present = self.matches_training_loss_log(self.line)
        # last training loss log is cast when training terminates
        # therefore the training_iteration after this loop gives the overall number of iterations
        # in this training loop run
        iters_key = (self.repair_step, self.backend_iterations_of_repair_step[-1])
        self.training_iterations_of_run[iters_key] = training_iteration_

    def parse(self, log_file):
        self.readline(log_file)
        while len(self.line) > 0:
            # handle lines that cause a state transition
            if self.matches(self.timeout_rx, self.line):
                self.experiment_result = ExperimentResult.TIMEOUT
                self.not_verified_properties = None
                break  # nothing interesting following
            elif self.matches(self.modified_network_saved, self.line):
                # this is an alternative final message for experiments that do not log final execution losses
                break
            elif self.matches(self.repair_successful_rx, self.line):
                self.experiment_result = ExperimentResult.SUCCESS
                self.not_verified_properties = []
                # continue: loss overview still follows and we want the final loss
            elif self.matches(self.backend_failure_rx, self.line):
                self.experiment_result = ExperimentResult.BACKEND_FAILURE
                self.not_verified_properties = []
                # continue: loss overview follows
            elif self.matches(self.max_iterations_failure_rx, self.line):
                self.experiment_result = ExperimentResult.MAX_ITERATIONS_FAILURE
                self.not_verified_properties = []
                # continue: loss overview follows
            elif self.matches(self.verification_problem_rx, self.line):
                self.experiment_result = ExperimentResult.VERIFICATION_PROBLEM
                self.not_verified_properties = [
                    prop_name.strip().replace("'", '')  # remove whitespace and enclosing ''
                    for prop_name in self.match.group('properties').split(',')
                ]
                # continue: also here final loss still follows
            elif self.matches(self.repair_network_iteration_rx, self.line):
                # marks the beginning of any repair step
                self.repair_step = int(self.match.group('repair_step'))
                self.backend_iterations_of_repair_step.append(0)
                self.training_loop_runs_of_repair_step.append([])
            elif self.matches(self.total_runtime_rx, self.line):
                self.total_runtime = self.match.group('time')
            elif self.matches(self.runtime_rx, self.line):
                self.runtimes[self.match.group('name')][self.repair_step] = self.match.group('time')
            elif self.matches(self.counterexample_art_rx, self.line):
                self.read_away_counterexample_art(log_file)
                continue  # read_away_counterexample_art has already read the next line
            elif self.matches_training_loss_log(self.line):
                self.training_loop_runs_of_repair_step[self.repair_step].append(self.training_loop_run)
                backend_iteration_ = self.backend_iterations_of_repair_step[-1]
                self.training_loop_runs_to_backend_iteration.append(backend_iteration_)
                self.read_away_training_loop(log_file)
                self.backend_iterations_of_repair_step[-1] += 1
                self.training_loop_run += 1
                continue
            # record initial and final loss
            elif self.matches(self.repair_level_loss_rx, self.line):
                if self.match.group('name') == 'Final':
                    self.final_loss = self.loss_string_to_dict(self.match.group('values'))
                    # the final loss is the last interesting thing in the log file
                    break
                elif self.match.group('name') == 'Initial' and self.initial_loss is None:
                    # ignore initial loss that is repeated at the end of the log file
                    self.initial_loss = self.loss_string_to_dict(self.match.group('values'))
            elif self.matches(self.network_name_rx, self.line):
                self.network_name = self.match.group('name')
            elif self.matches(self.verifier_rx, self.line):
                self.verifier = self.match.group('name')
            elif self.matches(self.falsifiers_rx, self.line):
                self.falsifiers = self.match.group('name')
            elif self.matches(self.verifier_exit_mode_rx, self.line):
                self.verifier_exit_mode = self.match.group('name')
            # match the repair backend
            else:
                for repair_backend, backend_rx in self.repair_backend_rxs.items():
                    if self.contains(backend_rx, self.line):
                        if self.repair_backend is not None:
                            warn('Found multiple repair backends in log file')
                        self.repair_backend = repair_backend
            self.readline(log_file)
        else:  # loop did not end with break => file was completely read but no final message contained
            self.experiment_result = ExperimentResult.ABORTED


def get_checkpoint_files_sorted(dir_: Path, file_name_rx):
    if not dir_.exists():
        print(f"No checkpoints found for experiment case: {experiment_case.name} in {dir_}")
        return None
    # sort the paths
    checkpoint_file_paths_ = {}
    for checkpoint_file_name in os.listdir(dir_):
        checkpoint_file_path = Path(dir_, checkpoint_file_name)
        match = file_name_rx.match(checkpoint_file_name)
        if match is None:
            print(f'Ignoring unknown file: {checkpoint_file_path}')
        else:
            keys = tuple(int(value) for value in match.groups())
            checkpoint_file_paths_[keys] = checkpoint_file_path
    # sort by keys (repair steps) and drop keys
    checkpoint_file_paths_ = sorted(checkpoint_file_paths_.items(), key=lambda item: item[0])
    checkpoint_file_paths_ = tuple(map(lambda item: item[1], checkpoint_file_paths_))
    return checkpoint_file_paths_


def startswith_matching_on_space(string_: str, prefix_: str):
    return string_.startswith(prefix_) and \
           (len(string_) == len(prefix_) or string_[len(prefix_)] == ' ')


def get_runtime_summary(runtimes_: Dict[str, List[float]], prefix_keys: List[str]):
    # contains: min, mean, median, max, sum of runtime of each category
    runtimes_recollected = {}
    for prefix_ in prefix_keys:
        runtimes_recollected[prefix_] = itertools.chain(
            *[times_ for key_, times_ in runtimes_.items() if startswith_matching_on_space(key_, prefix_)]
        )
    # aggregate: calculate: min, mean, median, max, sum of runtimes
    # will contain dictionaries with keys: task, min, mean, median, max, sum
    runtimes_aggregated = []
    for key_ in sorted(runtimes_recollected, key=len):
        runtimes_as_series = pandas.Series(runtimes_recollected[key_], dtype='float64')
        runtimes_aggregated.append(
            {
                'task': key_,
                'min': runtimes_as_series.min(),
                'mean': runtimes_as_series.mean(),
                'median': runtimes_as_series.median(),
                'max': runtimes_as_series.max(),
                'sum': runtimes_as_series.sum()
            }
        )
    runtimes_aggregated = pandas.DataFrame(runtimes_aggregated)
    return runtimes_aggregated


def load_network_and_do(network_file_path, action, network_working_copy, full_networks_stored):
    global counterexamples
    global data_loader
    global robustness_factory
    global robustness_verifier
    global robustness_falsifier
    global specification

    network: NeuralNetwork
    if full_networks_stored:
        network = torch.load(network_file_path, map_location=torch.device('cpu'))
    else:
        network = deepcopy(network_working_copy)
        network_state = torch.load(network_file_path, map_location=torch.device('cpu'))['model']
        network.load_state_dict(network_state)
    if action == 'counterexamples':
        # satisfaction_function > 0 => no violation (satisfaction)
        return [-min(0, prop_.satisfaction_function(torch.tensor(cx_).unsqueeze(0), network).item())
                for prop_, cx_ in counterexamples]
    elif action == 'test_set_acc':
        return [full_accuracy(network, data_loader, progress_bar=False)]
    elif action == 'robust_lb':
        fraction_verified, _ = check_local_robustness(network, data_loader, robustness_factory,
                                                      robustness_verifier, progress_bar=False)
        return [fraction_verified]
    elif action == 'robust_ub':
        _, fraction_falsified = check_local_robustness(network, data_loader, robustness_factory,
                                                       robustness_falsifier, progress_bar=False)
        return [1 - fraction_falsified]
    elif action == 'spec_sat':
        frac_sat = 0.0
        for inputs, _ in data_loader:
            props_sat = [prop_.property_satisfied(inputs, network) for prop_ in specification]
            spec_sat = torch.all(torch.stack(props_sat, dim=0), dim=0)
            frac_sat += torch.sum(spec_sat).item() / len(inputs)
        frac_sat = frac_sat / len(data_loader)
        return [frac_sat]


def process_networks(network_paths, action, network_working_copy, full_networks_stored, parallel=True) \
        -> List[List[float]]:
    if parallel:
        collected_arguments = [(path_, action, network_working_copy, full_networks_stored)
                               for path_ in network_paths]
        num_workers = os.cpu_count()
        chunk_size = len(collected_arguments) // num_workers
        if chunk_size == 0:
            chunk_size = None
        with mp.Pool(processes=num_workers) as pool:
            return pool.starmap(load_network_and_do, collected_arguments, chunksize=chunk_size)
    else:
        return [load_network_and_do(network_path, action, network_working_copy, full_networks_stored)
                for network_path in network_paths]


def calc_initial_violations(net_, cxs_) -> List[float]:
    if isinstance(net_, os.PathLike):
        net_ = torch.load(net_, map_location=torch.device('cpu'))
    return [-min(0, prop_.satisfaction_function(cx_.unsqueeze(0), net_).item()) for prop_, cx_ in cxs_]


def discrete_summary(series: pandas.Series):
    if len(series) == 0:
        return {}
    mode = series.mode()
    if len(mode) == 1:
        mode = int(mode)
    else:
        mode = list(mode)
    return OrderedDict([
        ('min', float(series.min())),
        ('quartile1', float(series.quantile(0.25))),
        ('median', float(series.median())),
        ('quartile3', float(series.quantile(0.75))),
        ('max', float(series.max())),
        ('mode', mode),
        ('mean', float(series.mean())),
        ('IQR', float(series.quantile(0.75) - series.quantile(0.25))),
    ])


def continuous_summary(series: pandas.Series):
    return OrderedDict([
        ('min', float(series.min())),
        ('quartile1', float(series.quantile(0.25))),
        ('median', float(series.median())),
        ('quartile3', float(series.quantile(0.75))),
        ('max', float(series.max())),
        ('mean', float(series.mean())),
        ('std', float(series.std())),
        ('IQR', float(series.quantile(0.75) - series.quantile(0.25))),
    ])


def data_frame_summary(data_frame: pandas.DataFrame, type_: str):
    summary_ = OrderedDict()
    for col_ in data_frame.columns:
        if type_ == 'continuous':
            col_summary = continuous_summary(data_frame[col_])
        elif type_ == 'discrete':
            col_summary = discrete_summary(data_frame[col_])
        else:
            raise ValueError(f'Unknown type: {type_}')
        summary_[col_] = col_summary
    return summary_


@dataclass
class ExperimentCase:
    name: str
    dir: os.PathLike
    # log file contents
    repair_backend: Optional[RepairBackend] = None
    repair_steps: Optional[int] = None
    initial_loss: Optional[Dict[str, float]] = None
    final_loss: Optional[Dict[str, float]] = None
    network_name: Optional[str] = None
    verifier: Optional[str] = None
    falsifiers: Optional[str] = None
    verifier_exit_mode: Optional[str] = None
    specification: Optional[Sequence[Property]] = None
    # columns: repair_step, backend_iteration, counterexamples_violated
    counterexamples_violated: Optional[pandas.DataFrame] = None
    # key: (repair_step, backend_iteration, training_iteration)
    training_losses: Optional[pandas.DataFrame] = None
    # key: (repair_step, backend_iteration)
    number_of_training_iterations_of_run: Optional[Dict[Tuple[int, int], int]] = None
    training_loop_runs_of_repair_step: Optional[List[List[int]]] = None
    # reverse lookup for repair steps (first property) and backend iterations (second element)
    training_loop_runs_to_repair_pos: Optional[List[Tuple[int, int]]] = None
    runtimes: Optional[Dict[str, Dict[int, float]]] = None
    total_runtime: Optional[float] = None
    experiment_result: Optional[ExperimentResult] = None
    not_verified_properties: Optional[List[str]] = None
    # checkpoint file paths
    # counterexamples: list of files sorted by repair step
    counterexample_files: Optional[Tuple[os.PathLike, ...]] = None
    checkpoint_files: Optional[Tuple[os.PathLike, ...]] = None
    # first three tuple elements give repair steps, backend iterations and training iterations
    training_checkpoint_files: Optional[List[Tuple[int, int, int, os.PathLike]]] = None
    # aggregated values
    aggregated_runtimes: Optional[pandas.DataFrame] = None
    # allows to match columns in counterexample_violations to repair steps
    counterexample_violations: Optional[pandas.DataFrame] = None
    counterexample_violations_at_generation: Optional[pandas.Series] = None
    counterexample_information: Optional[Dict] = None
    test_set_accuracies: Optional[pandas.DataFrame] = None
    test_set_robustness_lower_bound: Optional[pandas.DataFrame] = None
    test_set_robustness_upper_bound: Optional[pandas.DataFrame] = None
    test_set_specification_satisfied: Optional[pandas.DataFrame] = None
    random_sample_robustness_lower_bound: Optional[pandas.DataFrame] = None
    random_sample_robustness_upper_bound: Optional[pandas.DataFrame] = None
    random_sample_specification_satisfied: Optional[pandas.DataFrame] = None
    quasi_random_sample_robustness_lower_bound: Optional[pandas.DataFrame] = None
    quasi_random_sample_robustness_upper_bound: Optional[pandas.DataFrame] = None
    quasi_random_sample_specification_satisfied: Optional[pandas.DataFrame] = None
    final_test_set_accuracy: Optional[float] = None
    final_training_set_accuracy: Optional[float] = None
    # other values
    parameter_change_linf: Optional[float] = None
    parameter_change_l1: Optional[float] = None
    parameter_change_l2: Optional[float] = None
    num_counterexamples: Optional[int] = None
    num_newly_introduced_counterexamples: Optional[int] = None
    # list of network files used for calculating violations and other stuff
    # may only contain network states or full networks (property: network_files_store_full_network)
    # contains tuples: (total_training_iteration, repair_step, backend_iteration, training_iteration, network_path)
    network_files: Optional[List[Tuple[int, int, int, int, os.PathLike]]] = None
    network_files_store_full_network: Optional[bool] = None

    def copy_from_log_parser(self, parser: LogFileParser):
        self.repair_backend = parser.repair_backend
        # very important: +1; repair_steps is the total number, but repair_step counts from 0
        self.repair_steps = parser.repair_step + 1
        self.initial_loss = parser.initial_loss
        self.final_loss = parser.final_loss
        self.network_name = parser.network_name
        self.verifier = parser.verifier
        self.falsifiers = parser.falsifiers
        self.verifier_exit_mode = parser.verifier_exit_mode
        self.number_of_training_iterations_of_run = parser.training_iterations_of_run
        self.training_loop_runs_of_repair_step = parser.training_loop_runs_of_repair_step
        self.runtimes = parser.runtimes
        self.total_runtime = parser.total_runtime
        self.experiment_result = parser.experiment_result
        self.not_verified_properties = parser.not_verified_properties
        # create the reverse index for the training loop runs
        self.training_loop_runs_to_repair_pos = \
            [(-1, -1)] * sum(len(loop_runs) for loop_runs in self.training_loop_runs_of_repair_step)
        for repair_step_, loop_runs in enumerate(self.training_loop_runs_of_repair_step):
            for loop_run in loop_runs:
                backend_iteration_ = parser.training_loop_runs_to_backend_iteration[loop_run]
                self.training_loop_runs_to_repair_pos[loop_run] = (repair_step_, backend_iteration_)
        assert all(repair_step_ >= 0 and backend_iteration_ >= 0
                   for repair_step_, backend_iteration_ in self.training_loop_runs_to_repair_pos)

        # just put all the loss values in a data frame
        # will contain dicts with keys: repair_step, backend_iteration, training_iteration,
        #                               loss values...
        values_ = []
        # rp: repair step, bi: backend iteration, ti: training iteration, ci: combined iteration
        for (rs_, bi_, ti_), loss_values in parser.training_losses.items():
            element = {'repair_step': rs_, 'backend_iteration': bi_, 'training_iteration': ti_}
            element.update(loss_values)
            values_.append(element)
        self.training_losses = pandas.DataFrame(values_)

        # do the same for the binary counterexample violations
        # will contain dicts with keys: repair_step, backend_iteration, counterexample violated for each counterexample
        values_ = []
        for (rs_, bi_), num_violated in parser.counterexamples_violated.items():
            values_.append({'repair_step': rs_, 'backend_iteration': bi_, 'num_violated_counterexamples': num_violated})
        self.counterexamples_violated = pandas.DataFrame(values_)


if __name__ == '__main__':
    argument_parser = argparse.ArgumentParser('Analyse experiment output')
    argument_parser.add_argument('experiment_output_dir',
                                 help='The path to the output directory of the experiment to analyse.'
                                      'This directory has a timestamp as name. It contains a log file'
                                      'or subdirectories for each experiment case that then contain log files.')
    argument_parser.add_argument('--original_network', default=None,
                                 help='Descriptor of the original networks that were repaired in the experiment. '
                                      'Allowed values: full path to the network, '
                                      'ACASXu (network will be chosen dynamically based on the experiment case name).'
                                      'By default the network will be chosen by matching the experiment_output_dir '
                                      'to the known experiments. ')
    argument_parser.add_argument('--dataset', default=None,
                                 help='The dataset. Options: CollisionDetection, pimple_bowl, '
                                      'ACASXu, MNIST, CIFAR10, Adult, C-MAPSS. Used for determining which data to use. '
                                      'By default the dataset will be chosen by matching the experiment_output_dir '
                                      'to the known experiments. ')
    argument_parser.add_argument('--epoch_length', type=int, default=None,
                                 help='The length of one epoch. Used to convert epoch iterations to true iterations. '
                                      'Required whenever the log file contains epoch iterations. '
                                      'MNIST epoch length with batch size 64: 938. '
                                      'CIFAR10 epoch length with batch size 32: 1563. '
                                      'Adult epoch length with batch size 512: 59. '
                                      'C-MAPSS (window size 20) epoch length with batch size 512: 37. '
                                      'By default the epoch length is chosen by matching the experiment_output_dir '
                                      'to the known experiments. For MNIST, CIFAR10 and Adult the batch sizes given '
                                      'above are assumed. ')
    argument_parser.add_argument('--counterexample_violation', action='store_true',
                                 help='Calculate the violation of all counterexamples after each repair step.')
    argument_parser.add_argument('--counterexample_violation_at_generation', action='store_true',
                                 help='Calculate the counterexample violations at generation, '
                                      'i.e. when counterexamples are discovered.')
    argument_parser.add_argument('--count_introduced_counterexamples', action='store_true',
                                 help='Count how many counterexamples that were found are no counterexamples '
                                      'for the original network.')
    argument_parser.add_argument('--test_set_accuracy', action='store_true',
                                 help='Calculate the test set accuracy of all recorded networks.')
    argument_parser.add_argument('--test_set_robustness_lower_bound', action='store_true',
                                 help='Lower bound the fraction of inputs in the test set '
                                      'for which the network is robust. Calculate this for each recorded network. '
                                      'Invokes ERAN for the entire test set. '
                                      'Make sure to also pass --robustness_radius.')
    argument_parser.add_argument('--test_set_robustness_upper_bound', action='store_true',
                                 help='Upper bound the fraction of inputs in the test set '
                                      'for which the network is robust. Calculate this for each recorded network. '
                                      'Invokes the PGD attack for the entire test set. '
                                      'Make sure to also pass --robustness_radius.')
    argument_parser.add_argument('--random_sample_robustness_lower_bound', action='store_true',
                                 help='Lower bound the fraction of inputs in a random set of inputs '
                                      'for which the network is robust. Calculate this for each recorded network. '
                                      'Invokes ERAN for a large number of random samples. '
                                      'Make sure to also pass --robustness_radius.')
    argument_parser.add_argument('--random_sample_robustness_upper_bound', action='store_true',
                                 help='Upper bound the fraction of inputs in a random set of inputs '
                                      'for which the network is robust. Calculate this for each recorded network. '
                                      'Invokes the PGD attack for a large number of random samples. '
                                      'Make sure to also pass --robustness_radius.')
    argument_parser.add_argument('--quasi_random_sample_robustness_lower_bound', action='store_true',
                                 help='Lower bound the fraction of inputs in a grid of inputs '
                                      'for which the network is robust. Calculate this for each recorded network. '
                                      'Invokes ERAN for a large number of grid points. '
                                      'Make sure to also pass --robustness_radius.')
    argument_parser.add_argument('--quasi_random_sample_robustness_upper_bound', action='store_true',
                                 help='Upper bound the fraction of inputs in a grid of inputs '
                                      'for which the network is robust. Calculate this for each recorded network. '
                                      'Invokes the PGD attack for a large number of grid points. '
                                      'Make sure to also pass --robustness_radius.')
    argument_parser.add_argument('--robustness_radius', type=float, default=None,
                                 help='The radius for which to calculate robustness results.')
    argument_parser.add_argument('--test_set_specification_satisfied', action='store_true',
                                 help='Check property satisfaction on the test set. Calculate this for each'
                                      'recorded network. ')
    argument_parser.add_argument('--random_sample_specification_satisfied', action='store_true',
                                 help='Check property satisfaction on a random sample of inputs. '
                                      'Calculate this for each recorded network. ')
    argument_parser.add_argument('--quasi_random_sample_specification_satisfied', action='store_true',
                                 help='Check property satisfaction on a quasi-random sample (Sobol) of inputs. '
                                      'Calculate this for each recorded network. ')
#    argument_parser.add_argument('--specification', type=str, default=None,
#                                 help='The specification to use when calculating whether the specification is'
#                                      'satisfied. Can be inferred when there are counterexample checkpoints'
#                                      'and for certain datasets (Adult). Provide a string to a yaml file '
#                                      'containing the specification. The file may contain a single specification'
#                                      'of a mapping from experiment case names to specifications.')
    argument_parser.add_argument('--final_training_set_accuracy', action='store_true',
                                 help='Calculate the training set accuracy of the final modified networks.')
    argument_parser.add_argument('--rmi_final_accuracy', action='store_true',
                                 help='Calculate the training set accuracy of the final modified first layer '
                                      'rmi models '
                                      '(this is usually done already during the experiment).')
    argument_parser.add_argument('--final_test_set_accuracy', action='store_true',
                                 help='Calculate the test set accuracy of the final modified networks '
                                      '(this is usually done already during the experiment).')
    argument_parser.add_argument('--parameter_change', action='store_true',
                                 help='Calculate the parameter change between repaired and original networks.')
    argument_parser.add_argument('--detailed_runtimes', action='store_true',
                                 help='Besides the aggregated runtime data, also extract and store '
                                      'the runtimes per repair step.')
    argument_parser.add_argument('--batch_size', default=2048, type=int,
                                 help='The batch size when calculating dataset performance measures.')
    args = argument_parser.parse_args()

    if (args.test_set_robustness_upper_bound or args.test_set_robustness_lower_bound) and not args.robustness_radius:
        print("Supply a --robustness_radius for calculating robustness bounds.")
        exit(1)

    # get default values based on output dir
    dataset = args.dataset
    original_network_paths = args.original_network
    epoch_length = args.epoch_length
    if (
        'collision_detection_repair_4' in args.experiment_output_dir
        or 'collision_detection_repair_6' in args.experiment_output_dir
        or 'collision_detection_repair_7' in args.experiment_output_dir
    ):
        if dataset is None:
            dataset = 'CollisionDetection'
        if original_network_paths is None:
            original_network_paths = \
                Path('..', 'resources', 'collision_detection', 'CollisionDetection_ReLU_FFNN_1.pyt')
    elif 'collision_detection_repair_5' in args.experiment_output_dir:
        if dataset is None:
            dataset = 'CollisionDetection'
        if original_network_paths is None:
            original_network_paths = \
                Path('..', 'resources', 'collision_detection', 'CollisionDetection_ReLU_FFNN_3.pyt')
    elif 'pimple_bowl_repair_1' in args.experiment_output_dir:
        if dataset is None:
            dataset = 'pimple_bowl'
        if original_network_paths is None:
            # there are three options
            original_network_paths = {
                'tanh_50_2': Path('..', 'resources', 'pimple_bowl', 'pimple_bowl_tanh_50_2_network.pyt'),
                'tanh_100_2': Path('..', 'resources', 'pimple_bowl', 'pimple_bowl_tanh_100_2_network.pyt'),
                'tanh_500_2': Path('..', 'resources', 'pimple_bowl', 'pimple_bowl_tanh_500_2_network.pyt')
            }
    elif 'acasxu_repair_' in args.experiment_output_dir:
        if dataset is None:
            dataset = 'ACASXu'
        if original_network_paths is None:
            original_network_paths = 'ACASXu'
    elif 'mnist_repair_1' in args.experiment_output_dir \
            or 'mnist_repair_3' in args.experiment_output_dir:
        if dataset is None:
            dataset = 'MNIST'
        if original_network_paths is None:
            original_network_paths = {
                'eran_mnist_relu_3_50': Path('..', 'resources', 'mnist', 'eran_mnist_relu_3_50.pyt'),
                'eran_mnist_relu_5_100': Path('..', 'resources', 'mnist', 'eran_mnist_relu_5_100.pyt'),
                'eran_mnist_relu_9_100': Path('..', 'resources', 'mnist', 'eran_mnist_relu_9_100.pyt'),
                'eran_convSmallRELU__Point': Path('..', 'resources', 'mnist', 'eran_convSmallRELU__Point.pyt'),
                'mnist_cnn_1': Path("..", "resources", "mnist", "mnist_cnn_1.pyt"),
            }
        if epoch_length is None:
            # epoch_length = 938
            epoch_length = 1875
    elif 'cifar10_repair_1' in args.experiment_output_dir:
        if dataset is None:
            dataset = 'CIFAR10'
        if original_network_paths is None:
            original_network_paths = {
                'eran_cifar_conv_maxpool': Path('..', 'resources', 'cifar10', 'eran_cifar_conv_maxpool.pyt'),
                'eran_cifar_cnn_1': Path('..', 'resources', 'cifar10', 'cifar10_cnn_1.pyt'),
            }
        if epoch_length is None:
            epoch_length = 1563  # batch size 32
            # epoch_length = 391  # batch size 128
    elif 'adult_training_1' in args.experiment_output_dir:
        if dataset is None:
            dataset = 'Adult'
        if epoch_length is None:
            epoch_length = 59
    elif 'cmapss_repair_1' in args.experiment_output_dir:
        if dataset is None:
            dataset = 'CMAPSS20'
        if epoch_length is None:
            epoch_length = 37
        if original_network_paths is None:
            original_network_paths = {
                'cnn_window_20_1': Path("..", "resources", "cmapss", "cnn_window_20_1.pyt"),
                'fcnn_window_20_1': Path("..", "resources", "cmapss", "fcnn_window_20_1.pyt"),
            }
    elif 'ouroboros_rmi_repair_1' in args.experiment_output_dir:
        if dataset is None:
            dataset = "IntegerDataset"
        if epoch_length is None:
            epoch_length = 372
        if original_network_paths is None:
            original_network_paths = {
                f'rmi_10_{i}': Path("..", "resources", "ouroboros_rmi", f"rmi_10_{i}", "first_stage.pyt")
                for i in range(1, 11)
            }
            original_network_paths.update({
                f'rmi_304_{i}': Path("..", "resources", "ouroboros_rmi", f"rmi_304_{i}", "first_stage.pyt")
                for i in range(1, 51)
            })
    elif 'hcas_repair_3' in args.experiment_output_dir:
        if dataset is None:
            dataset = 'HCAS_PRA1_TAU00'
        if original_network_paths is None:
            print("INFO: assuming the ignore_speed network!")
            original_network_paths = Path('..', 'resources', 'hcas', 'HCAS_polar_v6_pra1_tau00_2021-04-08_23-51-43_ignore_speed.pyt')
        if epoch_length is None:
            epoch_length = 105

    acas_xu_network_rx = re.compile(r'net_(?P<i1>\d)_(?P<i2>\d)')

    def load_original_network(experiment_case_: ExperimentCase) -> NeuralNetwork:
        if original_network_paths == 'ACASXu':
            match_ = acas_xu_network_rx.search(experiment_case_.name)
            i1 = match_.group('i1')
            i2 = match_.group('i2')
            network_file = f'../resources/acasxu/ACASXU_run2a_{i1}_{i2}_batch_2000.nnet'
            return NeuralNetwork.load_from_nnet(network_file)
        elif isinstance(original_network_paths, os.PathLike) or isinstance(original_network_paths, str):
            return torch.load(original_network_paths, map_location=torch.device('cpu'))
        elif isinstance(original_network_paths, dict):
            # discover based on experiment case
            return torch.load(original_network_paths[experiment_case_.network_name],
                              map_location=torch.device('cpu'))
        elif Path(experiment_case_.dir, 'original_network.pyt').exists():
            return torch.load(Path(experiment_case_.dir, 'original_network.pyt'), map_location=torch.device('cpu'))
        else:
            raise ValueError('Could not infer original network path. '
                             'Please provide the --original_network command line argument')

    if dataset not in (None, 'ACASXu', "IntegerDataset"):
        dataset = datasets.Datasets.from_str(dataset)
    elif dataset is None:
        print("Proceeding without dataset. Some statistics can't be computed.")

    # read the directory structure first
    print('Discovering directory structure')
    output_dir = Path(args.experiment_output_dir)
    experiment_cases = []
    if 'log.txt' in os.listdir(output_dir):
        # top level dir contains log file => is experiment case output dir itself
        experiment_cases.append(ExperimentCase(name='main', dir=output_dir))
    else:
        for dir_element in natsorted(os.listdir(output_dir)):  # create a somewhat sensible ordering
            dir_element_path = Path(output_dir, dir_element)
            if dir_element_path.is_dir():
                experiment_cases.append(ExperimentCase(name=dir_element, dir=dir_element_path))
            else:
                print(f'Ignoring unknown directory element: {dir_element_path}')

    print('Parsing log files')
    for experiment_case in experiment_cases:
        log_file_path = Path(experiment_case.dir, 'log.txt')
        log_parser = LogFileParser(epoch_length)
        with open(log_file_path) as log_file_:
            log_parser.parse(log_file_)
        experiment_case.copy_from_log_parser(log_parser)

    print('Discovering counterexamples')
    cx_file_name_rx = re.compile(r'repair_step_(?P<repair_step>\d+)\.dill')
    for experiment_case in experiment_cases:
        counterexamples_dir_path = Path(experiment_case.dir, 'counterexamples')
        # only get the file names, loading all into memory is not advisable
        counterexample_file_paths = get_checkpoint_files_sorted(counterexamples_dir_path, cx_file_name_rx)
        if counterexample_file_paths is not None:
            experiment_case.counterexample_files = counterexample_file_paths

    print('Discovering network checkpoints')
    checkpoint_file_name_rx = re.compile(r'repair_step_(?P<repair_step>\d+)\.pyt')
    for experiment_case in experiment_cases:
        checkpoints_dir_path = Path(experiment_case.dir, 'checkpoints')
        # only get the file names, loading all into memory is not advisable
        checkpoint_file_paths = get_checkpoint_files_sorted(checkpoints_dir_path, checkpoint_file_name_rx)
        if checkpoint_file_paths is not None:
            experiment_case.checkpoint_files = checkpoint_file_paths

    print('Discovering training checkpoints')
    training_checkpoint_file_name_rx = re.compile(r'run_(?P<run>\d+)_iteration_(?P<iteration>\d+)\.pyt')
    for experiment_case in experiment_cases:
        training_checkpoints_dir_path = Path(experiment_case.dir, 'training_checkpoints')
        # only get the file names, loading all into memory is not advisable
        training_checkpoint_file_paths = get_checkpoint_files_sorted(training_checkpoints_dir_path,
                                                                     training_checkpoint_file_name_rx)
        if training_checkpoint_file_paths is not None:
            reorganised = []
            for path in training_checkpoint_file_paths:
                name_match = training_checkpoint_file_name_rx.match(path.name)
                run = int(name_match.group('run'))
                training_iteration = int(name_match.group('iteration'))
                repair_step, backend_iteration = experiment_case.training_loop_runs_to_repair_pos[run]
                reorganised.append((repair_step, backend_iteration, training_iteration, path))
            experiment_case.training_checkpoint_files = reorganised

    print('Discovering specifications')
    for experiment_case in experiment_cases:
        if 'adult_training_1' in args.experiment_output_dir:
            test_set: Adult = datasets.get_test_set(dataset)
            experiment_case.specification = [GlobalIndividualFairnessProperty(
                sensitive_attribute_indices=test_set.sensitive_column_indices,
                property_name="global individual fairness"
            )]
        elif experiment_case.counterexample_files is not None:
            print("INFO: obtaining properties from counterexample checkpoints. "
                  "Properties that are never violated during repair will not appear "
                  "in the results.")
            properties = set()
            for cx_path in experiment_case.counterexample_files:
                with open(cx_path, 'rb') as cx_file:
                    cxs = dill.load(cx_file)
                properties.union([prop for prop, _ in cxs])
            experiment_case.specification = list(properties)

    # aggregate runtimes
    print('Aggregating runtimes')
    # get all command keys and generate groups by matching prefixes (full words only, separated by space)
    runtime_keys = set(itertools.chain(*[case.runtimes.keys() for case in experiment_cases]))
    # add all prefixes that come up in more than one runtime_key
    runtime_keys_incl_prefixes: Set[str] = set()
    for full_key in runtime_keys:  # a full key is the longest possible prefix
        new_prefix = prefix = full_key
        new_matches = prefix_matches = {full_key}
        while len(prefix) > 0:  # no not process the empty word as prefix
            # shorten the prefix until the matched keys change
            while prefix_matches.issubset(new_matches):
                next_word_boundary = new_prefix.rfind(' ')
                if next_word_boundary <= 0:  # do not create the empty string
                    break
                new_prefix = prefix[:next_word_boundary]
                new_matches = set(key for key in runtime_keys if key.startswith(new_prefix))
                if new_prefix in runtime_keys_incl_prefixes:
                    break
            # the matched keys have changed
            if new_prefix in runtime_keys_incl_prefixes:
                # prefix and sub-prefixes already handled
                break
            runtime_keys_incl_prefixes.add(prefix)
            prefix = new_prefix
            prefix_matches = new_matches
    # prefixes may contain single word prefixes that only match one key. Remove those
    for prefix in list(runtime_keys_incl_prefixes):
        if prefix.find(' ') == -1 and sum(key.startswith(prefix) for key in runtime_keys) == 1:
            runtime_keys_incl_prefixes.remove(prefix)
    runtime_keys_incl_prefixes = runtime_keys_incl_prefixes.union(runtime_keys)
    runtime_keys_incl_prefixes: List[str] = sorted(runtime_keys_incl_prefixes, key=len)
    # first step: just collect all runtimes
    full_experiment_runtimes = defaultdict(list)
    for experiment_case in experiment_cases:
        runtimes_as_lists = {key: list(value.values()) for key, value in experiment_case.runtimes.items()}
        experiment_case.aggregated_runtimes = get_runtime_summary(runtimes_as_lists, runtime_keys_incl_prefixes)
        for runtime_key, runtimes in experiment_case.runtimes.items():
            full_experiment_runtimes[runtime_key].extend(runtimes.values())
    full_experiment_runtime_summary = get_runtime_summary(full_experiment_runtimes, runtime_keys_incl_prefixes)
    # print('Full runtime summary:')
    # print(full_experiment_runtime_summary)

    print("Choosing checkpoint networks.")
    for experiment_case in experiment_cases:
        # training checkpoint files and checkpoint files are usually not committed (to reduce data volume)
        # on other machines that the one on which the experiment was executed, these files will not be available
        if experiment_case.training_checkpoint_files is None and experiment_case.checkpoint_files is None:
            print(f'No checkpoints available for experiment case: {experiment_case.name}')
            continue

        if experiment_case.training_checkpoint_files is None or len(experiment_case.training_checkpoint_files) == 0:
            network_files = [(rs, rs, 0, 0, path) for rs, path in enumerate(experiment_case.checkpoint_files)]
            full_networks = True
        else:
            prev_ti_counter = 0
            last_rs = last_bi = last_ti = 0
            network_files = []
            # experiment_case.training_checkpoint_files need to be already sorted
            for rs, bi, ti, path in experiment_case.training_checkpoint_files:
                if rs != last_rs or bi != last_bi:
                    prev_ti_counter += last_ti
                network_files.append((prev_ti_counter + ti, rs, bi, ti, path))
                last_rs = rs
                last_bi = bi
                last_ti = ti
            full_networks = False
        experiment_case.network_files = network_files
        experiment_case.network_files_store_full_network = full_networks

    if args.counterexample_violation:
        print("Calculating counterexample violations")
        print("WARNING: these results may not be entirely accurate as storing may have changed the counterexamples! "
              "This may e.g. lead to counterexamples generated in the first run that are after storing no longer "
              "counterexamples for the initial network. ")
        progress_bar = tqdm(
            desc='experiment cases',
            total=sum(len(experiment_case.network_files) for experiment_case in experiment_cases)
        )
        for experiment_case in experiment_cases:
            original_network = load_original_network(experiment_case)
            # counterexamples for one network should fit in memory together
            counterexample_amounts = []  # allows mapping counterexamples to repair steps
            counterexamples: List[Tuple[Property, np.ndarray]] = []
            for cx_path in experiment_case.counterexample_files:
                with open(cx_path, 'rb') as cx_file:
                    cx = dill.load(cx_file)
                counterexample_amounts.append(len(cx))
                counterexamples.extend(cx)

            def get_rs_cx_index(i):
                cx_amount_sum = 0
                rs_ = 0
                for rs_ in range(len(counterexample_amounts)):
                    cx_amount_sum += counterexample_amounts[rs_]
                    if cx_amount_sum > i:
                        break
                sub_index = i - sum(counterexample_amounts[:rs_])
                return rs_, sub_index

            # this is a #network x #counterexamples list of lists
            violations: List[List[float]] = process_networks(
                map(lambda t: t[4], tqdm(experiment_case.network_files, desc=experiment_case.name)),
                'counterexamples',
                original_network,
                experiment_case.network_files_store_full_network
            )
            progress_bar.update(len(experiment_case.network_files))
            violations: pandas.DataFrame = pandas.DataFrame([
                dict(
                    (('total_training_iteration', tti), ('repair_step', rs),
                     ('backend_iteration', bi), ('training_iteration', ti))
                    + tuple(('cx_{}_{}'.format(*get_rs_cx_index(i)), val) for i, val in enumerate(cx_viols))
                )
                for (tti, rs, bi, ti, _), cx_viols in zip(experiment_case.network_files, violations)
            ])
            experiment_case.counterexample_violations = violations

            cx_info = OrderedDict()
            for i, (prop, cx) in enumerate(counterexamples):
                rs, sub_index = get_rs_cx_index(i)
                cx_key = f'cx_{rs}_{sub_index}'
                cx_info[cx_key] = OrderedDict([
                    ('property', prop.property_name),
                    ('repair_step', rs),
                    ('index_in_repair_step', sub_index)
                ])
            experiment_case.counterexample_information = cx_info

    if args.counterexample_violation_at_generation:
        print("Calculating counterexample violations at generation")
        print("WARNING: these results may not be entirely accurate as storing may have changed the counterexamples! "
              "This may e.g. lead to counterexamples generated in the first run that are after storing no longer "
              "counterexamples for the initial network. ")
        progress_bar = tqdm(
            desc='experiment cases',
            total=sum(experiment_case.repair_steps for experiment_case in experiment_cases)
        )
        for experiment_case in experiment_cases:
            original_network = load_original_network(experiment_case)
            # counterexamples for one network should fit in memory together
            counterexample_amounts = []  # allows mapping counterexamples to repair steps
            counterexamples: List[Tuple[Property, np.ndarray]] = []
            for cx_path in experiment_case.counterexample_files:
                with open(cx_path, 'rb') as cx_file:
                    cx = dill.load(cx_file)
                counterexample_amounts.append(len(cx))
                counterexamples.extend(cx)

            def get_rs_cx_index(i):
                cx_amount_sum = 0
                rs_ = 0
                for rs_ in range(len(counterexample_amounts)):
                    cx_amount_sum += counterexample_amounts[rs_]
                    if cx_amount_sum > i:
                        break
                sub_index = i - sum(counterexample_amounts[:rs_])
                return rs_, sub_index

            # also calculate counterexamples at generation
            with TemporaryDirectory() as original_network_temp_dir:
                original_network_temp_file = Path(original_network_temp_dir, 'network.pyt')
                torch.save(original_network, original_network_temp_file)
                networks_at_start_of_repair_step_files = \
                    (original_network_temp_file,) + experiment_case.checkpoint_files[:-1]
                violations_at_generation: List[List[float]] = process_networks(
                    map(lambda t: t, tqdm(networks_at_start_of_repair_step_files, desc=experiment_case.name)),
                    'counterexamples',
                    original_network, True,  # checkpoint files always contain full networks
                    parallel=False
                )
                progress_bar.update(experiment_case.repair_steps)
                # now we have actually calculated too many values
                # keep only the violations in the repair step a counterexample is generated
                violations_at_generation: pandas.Series = pandas.Series(
                    [violations_at_generation[get_rs_cx_index(i)[0]][i] for i in range(len(counterexamples))],
                    index=['cx_{}_{}'.format(*get_rs_cx_index(i)) for i in range(len(counterexamples))],
                    dtype='float64'
                )
                experiment_case.counterexample_violations_at_generation = violations_at_generation

            if experiment_case.counterexample_information is None:
                cx_info = OrderedDict()
                for i, (prop, cx) in enumerate(counterexamples):
                    rs, sub_index = get_rs_cx_index(i)
                    cx_key = f'cx_{rs}_{sub_index}'
                    cx_info[cx_key] = OrderedDict([
                        ('property', prop.property_name),
                        ('repair_step', rs),
                        ('index_in_repair_step', sub_index)
                    ])
                experiment_case.counterexample_information = cx_info

    if args.final_test_set_accuracy:
        print("Calculating final test set accuracies")
        progress_bar = tqdm(desc='experiment cases', total=len(experiment_cases))
        test_set = datasets.get_test_set(dataset)
        test_set_loader = DataLoader(test_set, batch_size=args.batch_size, num_workers=os.cpu_count())
        for experiment_case in experiment_cases:
            repaired_network_path = Path(experiment_case.dir, 'repaired_network.pyt')
            if repaired_network_path.exists():
                repaired_network = torch.load(repaired_network_path, map_location=torch.device('cpu'))
                test_set_accuracy = full_accuracy(repaired_network, test_set_loader, progress_bar=True)
                experiment_case.final_test_set_accuracy = test_set_accuracy
            progress_bar.update()

    if args.final_training_set_accuracy:
        print("Calculating final training set accuracies")
        progress_bar = tqdm(desc='experiment cases', total=len(experiment_cases))
        training_set = datasets.get_training_set(dataset)
        training_set_loader = DataLoader(training_set, batch_size=args.batch_size, num_workers=os.cpu_count())
        for experiment_case in experiment_cases:
            repaired_network_path = Path(experiment_case.dir, 'repaired_network.pyt')
            if repaired_network_path.exists():
                repaired_network = torch.load(repaired_network_path, map_location=torch.device('cpu'))
                training_set_accuracy = full_accuracy(repaired_network, training_set_loader, progress_bar=True)
                experiment_case.final_training_set_accuracy = training_set_accuracy
            progress_bar.update()

    if args.rmi_final_accuracy:
        print("Calculating final RMI training set accuracies")
        progress_bar = tqdm(desc='experiment cases', total=len(experiment_cases))
        for experiment_case in experiment_cases:
            rmi_params_path = Path("..", "resources", "ouroboros_rmi", experiment_case.network_name, "params.yaml")
            with open(rmi_params_path, "rt") as file:
                rmi_params = ruamel.yaml.safe_load(file)
            repaired_network_path = Path(experiment_case.dir, 'repaired_network.pyt')
            dataset = integer_dataset(
                size=rmi_params["dataset"]["size"],
                distribution=rmi_params["dataset"]["distribution"],
                maximum=rmi_params["dataset"]["maximum"],
                seed=rmi_params["dataset"]["seed"],
            )
            full_loader = DataLoader(dataset, batch_size=len(dataset), num_workers=0)
            second_stage_size = rmi_params["second_stage_size"]
            partition_size = len(dataset) // second_stage_size
            partitions = np.array_split(np.arange(len(dataset)), second_stage_size)
            if repaired_network_path.exists():
                repaired_network = torch.load(repaired_network_path, map_location=torch.device('cpu'))
                keys, pos = next(iter(full_loader))
                parts = torch.div(pos, partition_size, rounding_mode='floor')
                keys = keys.unsqueeze(-1)
                parts = parts.unsqueeze(-1).float()
                preds = repaired_network(keys.float())
                accuracy = (parts == preds.round()).float().mean() * 100
                experiment_case.final_training_set_accuracy = accuracy.item()
            progress_bar.update()

    if args.test_set_accuracy:
        print("Calculating test set accuracies")
        progress_bar = tqdm(
            desc='experiment cases',
            total=sum(len(experiment_case.network_files) for experiment_case in experiment_cases)
        )
        for experiment_case in experiment_cases:
            original_network = load_original_network(experiment_case)
            test_set = datasets.get_test_set(dataset)
            data_loader = DataLoader(test_set, batch_size=args.batch_size, num_workers=4)
            # this is a #network x 1 list of lists
            test_set_accuracies: List[List[float]] = process_networks(
                map(lambda t: t[4], tqdm(experiment_case.network_files, desc=experiment_case.name)),
                'test_set_acc',
                original_network,
                experiment_case.network_files_store_full_network,
                parallel=False
            )
            progress_bar.update(len(experiment_case.network_files))
            test_set_accuracies: pandas.DataFrame = pandas.DataFrame([
                dict(
                    (('total_training_iteration', tti), ('repair_step', rs),
                     ('backend_iteration', bi), ('training_iteration', ti),
                     ('test_set_accuracy', acc[0]))
                )
                for (tti, rs, bi, ti, _), acc in zip(experiment_case.network_files, test_set_accuracies)
            ])
            experiment_case.test_set_accuracies = test_set_accuracies

    if args.test_set_robustness_lower_bound:
        print("Calculating test set robustness lower bound")
        assert args.robustness_radius is not None, "Calculating robustness requires a robustness radius"
        progress_bar = tqdm(
            desc='experiment cases',
            total=sum(len(experiment_case.network_files) for experiment_case in experiment_cases)
        )
        for experiment_case in experiment_cases:
            original_network = load_original_network(experiment_case)
            test_set = datasets.get_test_set(dataset)
            data_loader = DataLoader(test_set, batch_size=args.batch_size, num_workers=4)
            robustness_factory = RobustnessPropertyFactory(eps=args.robustness_radius)
            # use_milp=False => only DeepPoly certification
            robustness_verifier = ERAN(use_acasxu_style=False, use_milp=False, progress_bar=False)
            # this is a #network x 1 list of lists
            test_set_fraction_certified: List[List[float]] = process_networks(
                map(lambda t: t[4], tqdm(experiment_case.network_files, desc=experiment_case.name)),
                'robust_lb',
                original_network,
                experiment_case.network_files_store_full_network,
                parallel=False
            )
            progress_bar.update(len(experiment_case.network_files))
            test_set_fraction_certified: pandas.DataFrame = pandas.DataFrame([
                dict(
                    (('total_training_iteration', tti), ('repair_step', rs),
                     ('backend_iteration', bi), ('training_iteration', ti),
                     ('fraction_robustness_lower_bound', acc[0]))
                )
                for (tti, rs, bi, ti, _), acc in zip(experiment_case.network_files, test_set_fraction_certified)
            ])
            experiment_case.test_set_robustness_lower_bound = test_set_fraction_certified

    if args.test_set_robustness_upper_bound:
        print("Calculating test set robustness upper bound")
        assert args.robustness_radius is not None, "Calculating robustness requires a robustness radius"
        progress_bar = tqdm(
            desc='experiment cases',
            total=sum(len(experiment_case.network_files) for experiment_case in experiment_cases)
        )
        for experiment_case in experiment_cases:
            original_network = load_original_network(experiment_case)
            test_set = datasets.get_test_set(dataset)
            data_loader = DataLoader(test_set, batch_size=args.batch_size, num_workers=4)
            robustness_factory = RobustnessPropertyFactory(eps=args.robustness_radius)
            robustness_falsifier = ProjectedGradientDescentAttack(progress_bar=False)
            # this is a #network x 1 list of lists
            test_set_fraction_disproven: List[List[float]] = process_networks(
                map(lambda t: t[4], tqdm(experiment_case.network_files, desc=experiment_case.name)),
                'robust_ub',
                original_network,
                experiment_case.network_files_store_full_network,
                parallel=False
            )
            progress_bar.update(len(experiment_case.network_files))
            test_set_fraction_disproven: pandas.DataFrame = pandas.DataFrame([
                dict(
                    (('total_training_iteration', tti), ('repair_step', rs),
                     ('backend_iteration', bi), ('training_iteration', ti),
                     ('fraction_robustness_lower_bound', acc[0]))
                )
                for (tti, rs, bi, ti, _), acc in zip(experiment_case.network_files, test_set_fraction_disproven)
            ])
            experiment_case.test_set_robustness_upper_bound = test_set_fraction_disproven

    if args.random_sample_robustness_lower_bound:
        print("Calculating random sample robustness lower bound")
        assert args.robustness_radius is not None, "Calculating robustness requires a robustness radius"
        progress_bar = tqdm(
            desc='experiment cases',
            total=sum(len(experiment_case.network_files) for experiment_case in experiment_cases)
        )
        test_set = datasets.get_test_set(dataset)
        rng = np.random.default_rng(9201705)  # IMPORTANT: seed needs to agree with seed used for upper bounds
        for experiment_case in experiment_cases:
            original_network = load_original_network(experiment_case)
            random_sample = sample_from_normal_in_range(
                locs=original_network.means_inputs,
                scales=original_network.ranges_inputs,
                mins=original_network.mins,
                maxes=original_network.maxes,
                num_rows=len(test_set),
                rng=rng,
            )
            random_sample = torch.utils.data.TensorDataset(random_sample, torch.zeros((len(test_set), 1)))
            data_loader = DataLoader(random_sample, batch_size=args.batch_size, num_workers=4)
            robustness_factory = RobustnessPropertyFactory(eps=args.robustness_radius)
            # use_milp=False => only DeepPoly certification
            robustness_verifier = ERAN(use_acasxu_style=False, use_milp=False, progress_bar=False)
            # this is a #network x 1 list of lists
            rand_fraction_certified: List[List[float]] = process_networks(
                map(lambda t: t[4], tqdm(experiment_case.network_files, desc=experiment_case.name)),
                'robust_lb',
                original_network,
                experiment_case.network_files_store_full_network,
                parallel=False
            )
            progress_bar.update(len(experiment_case.network_files))
            rand_fraction_certified: pandas.DataFrame = pandas.DataFrame([
                dict(
                    (('total_training_iteration', tti), ('repair_step', rs),
                     ('backend_iteration', bi), ('training_iteration', ti),
                     ('fraction_robustness_lower_bound', acc[0]))
                )
                for (tti, rs, bi, ti, _), acc in zip(experiment_case.network_files, rand_fraction_certified)
            ])
            experiment_case.random_sample_robustness_lower_bound = rand_fraction_certified

    if args.random_sample_robustness_upper_bound:
        print("Calculating random sample robustness upper bound")
        assert args.robustness_radius is not None, "Calculating robustness requires a robustness radius"
        progress_bar = tqdm(
            desc='experiment cases',
            total=sum(len(experiment_case.network_files) for experiment_case in experiment_cases)
        )
        test_set = datasets.get_test_set(dataset)
        rng = np.random.default_rng(9201705)  # IMPORTANT: seed needs to agree with seed used for lower bounds
        for experiment_case in experiment_cases:
            original_network = load_original_network(experiment_case)
            random_sample = sample_from_normal_in_range(
                locs=original_network.means_inputs,
                scales=original_network.ranges_inputs,
                mins=original_network.mins,
                maxes=original_network.maxes,
                num_rows=len(test_set),
                rng=rng,
            )
            random_sample_labels = original_network()
            random_sample = torch.utils.data.TensorDataset(random_sample, torch.zeros((len(test_set), 1)))
            data_loader = DataLoader(random_sample, batch_size=args.batch_size, num_workers=4)
            robustness_factory = RobustnessPropertyFactory(eps=args.robustness_radius)
            robustness_falsifier = ProjectedGradientDescentAttack(progress_bar=False)
            # this is a #network x 1 list of lists
            quasi_rand_fraction_disproven: List[List[float]] = process_networks(
                map(lambda t: t[4], tqdm(experiment_case.network_files, desc=experiment_case.name)),
                'robust_ub',
                original_network,
                experiment_case.network_files_store_full_network,
                parallel=False
            )
            progress_bar.update(len(experiment_case.network_files))
            quasi_rand_fraction_disproven: pandas.DataFrame = pandas.DataFrame([
                dict(
                    (('total_training_iteration', tti), ('repair_step', rs),
                     ('backend_iteration', bi), ('training_iteration', ti),
                     ('fraction_robustness_lower_bound', acc[0]))
                )
                for (tti, rs, bi, ti, _), acc in zip(experiment_case.network_files, quasi_rand_fraction_disproven)
            ])
            experiment_case.random_sample_robustness_upper_bound = quasi_rand_fraction_disproven

    if args.quasi_random_sample_robustness_lower_bound:
        print("Calculating quasi-random sample robustness lower bound")
        assert args.robustness_radius is not None, "Calculating robustness requires a robustness radius"
        progress_bar = tqdm(
            desc='experiment cases',
            total=sum(len(experiment_case.network_files) for experiment_case in experiment_cases)
        )
        test_set = datasets.get_test_set(dataset)
        for experiment_case in experiment_cases:
            original_network = load_original_network(experiment_case)
            sobol_dataset = SobolDataset(
                number_of_samples=len(test_set), dimension=prod(test_set[0][0].shape),
                target_fn=lambda *args: torch.tensor(0),
                seed=843258791001
            )
            data_loader = DataLoader(sobol_dataset, batch_size=args.batch_size, num_workers=1)
            robustness_factory = RobustnessPropertyFactory(eps=args.robustness_radius)
            # use_milp=False => only DeepPoly certification
            robustness_verifier = ERAN(use_acasxu_style=False, use_milp=False, progress_bar=False)
            # this is a #network x 1 list of lists
            quasi_rand_fraction_certified: List[List[float]] = process_networks(
                map(lambda t: t[4], tqdm(experiment_case.network_files, desc=experiment_case.name)),
                'robust_lb',
                original_network,
                experiment_case.network_files_store_full_network,
                parallel=False
            )
            progress_bar.update(len(experiment_case.network_files))
            quasi_rand_fraction_certified: pandas.DataFrame = pandas.DataFrame([
                dict(
                    (('total_training_iteration', tti), ('repair_step', rs),
                     ('backend_iteration', bi), ('training_iteration', ti),
                     ('fraction_robustness_lower_bound', acc[0]))
                )
                for (tti, rs, bi, ti, _), acc in zip(experiment_case.network_files, quasi_rand_fraction_certified)
            ])
            experiment_case.quasi_random_sample_robustness_lower_bound = quasi_rand_fraction_certified

    if args.quasi_random_sample_robustness_upper_bound:
        print("Calculating quasi-random sample robustness upper bound")
        assert args.robustness_radius is not None, "Calculating robustness requires a robustness radius"
        progress_bar = tqdm(
            desc='experiment cases',
            total=sum(len(experiment_case.network_files) for experiment_case in experiment_cases)
        )
        test_set = datasets.get_test_set(dataset)
        rng = np.random.default_rng(9201705)  # IMPORTANT: seed needs to agree with seed used for lower bounds
        for experiment_case in experiment_cases:
            original_network = load_original_network(experiment_case)
            sobol_dataset = SobolDataset(
                number_of_samples=len(test_set), dimension=prod(test_set[0][0].shape),
                target_fn=lambda *args: torch.tensor(0),
                seed=843258791001
            )
            data_loader = DataLoader(sobol_dataset, batch_size=args.batch_size, num_workers=1)
            robustness_factory = RobustnessPropertyFactory(eps=args.robustness_radius)
            robustness_falsifier = ProjectedGradientDescentAttack(progress_bar=False)
            # this is a #network x 1 list of lists
            quasi_rand_fraction_disproven: List[List[float]] = process_networks(
                map(lambda t: t[4], tqdm(experiment_case.network_files, desc=experiment_case.name)),
                'robust_ub',
                original_network,
                experiment_case.network_files_store_full_network,
                parallel=False
            )
            progress_bar.update(len(experiment_case.network_files))
            quasi_rand_fraction_disproven: pandas.DataFrame = pandas.DataFrame([
                dict(
                    (('total_training_iteration', tti), ('repair_step', rs),
                     ('backend_iteration', bi), ('training_iteration', ti),
                     ('fraction_robustness_lower_bound', acc[0]))
                )
                for (tti, rs, bi, ti, _), acc in zip(experiment_case.network_files, quasi_rand_fraction_disproven)
            ])
            experiment_case.quasi_random_sample_robustness_upper_bound = quasi_rand_fraction_disproven

    if args.test_set_specification_satisfied:
        print("Calculating specification satisfaction on test set")
        progress_bar = tqdm(
            desc='experiment cases',
            total=sum(len(experiment_case.network_files) for experiment_case in experiment_cases)
        )
        test_set = datasets.get_test_set(dataset)
        for experiment_case in experiment_cases:
            specification = experiment_case.specification
            original_network = load_original_network(experiment_case)
            data_loader = DataLoader(test_set, batch_size=args.batch_size, num_workers=4)
            # this is a #network x 1 list of lists
            test_set_fraction_spec_satisfied: List[List[float]] = process_networks(
                map(lambda t: t[4], tqdm(experiment_case.network_files, desc=experiment_case.name)),
                'spec_sat',
                original_network,
                experiment_case.network_files_store_full_network,
                parallel=False
            )
            progress_bar.update(len(experiment_case.network_files))
            test_set_fraction_spec_satisfied: pandas.DataFrame = pandas.DataFrame([
                dict(
                    (('total_training_iteration', tti), ('repair_step', rs),
                     ('backend_iteration', bi), ('training_iteration', ti),
                     ('fraction_specification_satisfied', frac[0]))
                )
                for (tti, rs, bi, ti, _), frac in zip(experiment_case.network_files, test_set_fraction_spec_satisfied)
            ])
            experiment_case.test_set_specification_satisfied = test_set_fraction_spec_satisfied

    if args.random_sample_specification_satisfied:
        print("Calculating specification satisfaction on random sample")
        progress_bar = tqdm(
            desc='experiment cases',
            total=sum(len(experiment_case.network_files) for experiment_case in experiment_cases)
        )
        test_set = datasets.get_test_set(dataset)
        rng = np.random.default_rng(9201705)
        for experiment_case in experiment_cases:
            specification = experiment_case.specification
            original_network = load_original_network(experiment_case)
            random_sample = sample_from_normal_in_range(
                locs=original_network.means_inputs,
                scales=original_network.ranges_inputs,
                mins=original_network.mins,
                maxes=original_network.maxes,
                num_rows=len(test_set),
                rng=rng,
                show_progress=True
            )
            random_sample = torch.utils.data.TensorDataset(random_sample, torch.zeros((len(test_set), 1)))
            data_loader = DataLoader(random_sample, batch_size=args.batch_size, num_workers=4)
            # this is a #network x 1 list of lists
            random_sample_fraction_spec_satisfied: List[List[float]] = process_networks(
                map(lambda t: t[4], tqdm(experiment_case.network_files, desc=experiment_case.name)),
                'spec_sat',
                original_network,
                experiment_case.network_files_store_full_network,
                parallel=False
            )
            progress_bar.update(len(experiment_case.network_files))
            random_sample_fraction_spec_satisfied: pandas.DataFrame = pandas.DataFrame([
                dict(
                    (('total_training_iteration', tti), ('repair_step', rs),
                     ('backend_iteration', bi), ('training_iteration', ti),
                     ('fraction_specification_satisfied', frac[0]))
                )
                for (tti, rs, bi, ti, _), frac
                in zip(experiment_case.network_files, random_sample_fraction_spec_satisfied)
            ])
            experiment_case.random_sample_specification_satisfied = random_sample_fraction_spec_satisfied

    if args.quasi_random_sample_specification_satisfied:
        print("Calculating specification satisfaction on quasi-random sample")
        progress_bar = tqdm(
            desc='experiment cases',
            total=sum(len(experiment_case.network_files) for experiment_case in experiment_cases)
        )
        test_set = datasets.get_test_set(dataset)
        rng = np.random.default_rng(9201705)
        for experiment_case in experiment_cases:
            specification = experiment_case.specification
            original_network = load_original_network(experiment_case)
            sobol_dataset = SobolDataset(
                number_of_samples=len(test_set), dimension=prod(test_set[0][0].shape),
                target_fn=lambda *args: torch.tensor(0),
                seed=843258791001
            )
            data_loader = DataLoader(sobol_dataset, batch_size=args.batch_size, num_workers=1)
            # this is a #network x 1 list of lists
            random_sample_fraction_spec_satisfied: List[List[float]] = process_networks(
                map(lambda t: t[4], tqdm(experiment_case.network_files, desc=experiment_case.name)),
                'spec_sat',
                original_network,
                experiment_case.network_files_store_full_network,
                parallel=False
            )
            progress_bar.update(len(experiment_case.network_files))
            random_sample_fraction_spec_satisfied: pandas.DataFrame = pandas.DataFrame([
                dict(
                    (('total_training_iteration', tti), ('repair_step', rs),
                     ('backend_iteration', bi), ('training_iteration', ti),
                     ('fraction_specification_satisfied', frac[0]))
                )
                for (tti, rs, bi, ti, _), frac
                in zip(experiment_case.network_files, random_sample_fraction_spec_satisfied)
            ])
            experiment_case.quasi_random_sample_specification_satisfied = random_sample_fraction_spec_satisfied

    if args.parameter_change:
        print("Calculating parameter change")
        for experiment_case in experiment_cases:
            original_network = load_original_network(experiment_case)
            repaired_network_path = Path(experiment_case.dir, 'repaired_network.pyt')
            if repaired_network_path.exists():
                repaired_network = torch.load(repaired_network_path, map_location=torch.device('cpu'))
                param_diff = [abs(old - new) for new, old
                              in zip(original_network.parameters(), repaired_network.parameters())]
                l1 = [torch.sum(x) for x in param_diff]
                l1 = torch.stack(l1).sum().item()
                l2 = (x ** 2 for x in param_diff)
                l2 = [torch.sum(x) for x in l2]
                l2 = torch.stack(l2).sum().sqrt().item()
                linf = [torch.max(x) for x in param_diff]
                linf = torch.stack(linf).max().item()

                experiment_case.parameter_change_l1 = l1
                experiment_case.parameter_change_l2 = l2
                experiment_case.parameter_change_linf = linf

    if args.count_introduced_counterexamples:
        print("Counting newly introduced counterexamples")
        for experiment_case in experiment_cases:
            original_network = load_original_network(experiment_case)
            # counterexamples for one network should fit in memory together
            counterexamples: List[Tuple[Property, np.ndarray]] = []
            for cx_path in experiment_case.counterexample_files:
                with open(cx_path, 'rb') as cx_file:
                    cx = dill.load(cx_file)
                counterexamples.extend(cx)

            if len(counterexamples) > 0:  # in case of a timeout in the first repair step no counterexamples are stored
                experiment_case.num_counterexamples = len(counterexamples)
                experiment_case.num_newly_introduced_counterexamples = sum(
                    prop.property_satisfied(torch.as_tensor(cx_inputs).unsqueeze(0), original_network).item()
                    for prop, cx_inputs in counterexamples
                )

    print('Storing individual experiment case results.')
    # store results in two formats: all dataframes in an hdf5 file and everything else in a yaml file
    # yaml files are very useful for manual inspection, so most aggregated information is put into the yaml file

    # dump ordered maps as simple YAML mappings from:
    # https://stackoverflow.com/questions/53874345/how-do-i-dump-an-ordereddict-out-as-a-yaml-file
    class OMPlainRepr(ruamel.yaml.RoundTripRepresenter):
        pass

    ruamel.yaml.add_representer(OrderedDict, OMPlainRepr.represent_dict, representer=OMPlainRepr)
    # also dump defaultdicts are ordinary mappings
    ruamel.yaml.add_representer(defaultdict, OMPlainRepr.represent_dict, representer=OMPlainRepr)
    yaml = ruamel.yaml.YAML(typ='safe')
    yaml.Representer = OMPlainRepr
    yaml.indent = 4
    yaml.compact_seq_map = True
    yaml.sequence_dash_offset = 0

    # keep results that were calculated in earlier runs
    yaml_file_path = Path(output_dir, 'results.yaml')
    if yaml_file_path.exists():
        with open(Path(output_dir, 'results.yaml'), 'rt') as yaml_results_file:
            yaml_results = OrderedDict(yaml.load(yaml_results_file))
    else:
        yaml_results = OrderedDict()
    if 'cases' not in yaml_results:
        yaml_results['cases'] = OrderedDict()
    hdf5_results = {}  # hierarchic keys to data frames

    for experiment_case in experiment_cases:
        # dataframes stored in the hdf5 file
        if experiment_case.training_losses is not None and len(experiment_case.training_losses) > 0:
            hdf5_results[f'cases/{experiment_case.name}/training_losses'] = experiment_case.training_losses
        if experiment_case.aggregated_runtimes is not None:
            hdf5_results[f'cases/{experiment_case.name}/runtimes'] = experiment_case.aggregated_runtimes
        if experiment_case.counterexample_violations is not None:
            hdf5_results[f'cases/{experiment_case.name}/counterexample_violations'] = \
                experiment_case.counterexample_violations
        if experiment_case.counterexample_violations_at_generation is not None:
            hdf5_results[f'cases/{experiment_case.name}/counterexample_violations_at_generation'] = \
                experiment_case.counterexample_violations_at_generation
        # counterexamples_violated only tells whether a counterexample is violated after a backend iteration
        if experiment_case.counterexamples_violated is not None:
            hdf5_results[f'cases/{experiment_case.name}/counterexamples_violated_after_backend_iteration'] = \
                experiment_case.counterexamples_violated
        if experiment_case.test_set_accuracies is not None:
            hdf5_results[f'cases/{experiment_case.name}/test_set_accuracies'] = experiment_case.test_set_accuracies
        if experiment_case.test_set_robustness_lower_bound is not None:
            hdf5_results[f'cases/{experiment_case.name}/test_set_robustness_lower_bound'] = \
                experiment_case.test_set_robustness_lower_bound
        if experiment_case.test_set_robustness_upper_bound is not None:
            hdf5_results[f'cases/{experiment_case.name}/test_set_robustness_upper_bound'] = \
                experiment_case.test_set_robustness_upper_bound
        if experiment_case.test_set_specification_satisfied is not None:
            hdf5_results[f'cases/{experiment_case.name}/test_set_specification_satisfied'] = \
                experiment_case.test_set_specification_satisfied
        if experiment_case.random_sample_robustness_lower_bound is not None:
            hdf5_results[f'cases/{experiment_case.name}/random_sample_robustness_lower_bound'] = \
                experiment_case.random_sample_robustness_lower_bound
        if experiment_case.random_sample_robustness_upper_bound is not None:
            hdf5_results[f'cases/{experiment_case.name}/random_sample_robustness_upper_bound'] = \
                experiment_case.random_sample_robustness_upper_bound
        if experiment_case.random_sample_specification_satisfied is not None:
            hdf5_results[f'cases/{experiment_case.name}/random_sample_specification_satisfied'] = \
                experiment_case.random_sample_specification_satisfied
        if experiment_case.quasi_random_sample_robustness_lower_bound is not None:
            hdf5_results[f'cases/{experiment_case.name}/quasi_random_sample_robustness_lower_bound'] = \
                experiment_case.quasi_random_sample_robustness_lower_bound
        if experiment_case.quasi_random_sample_robustness_upper_bound is not None:
            hdf5_results[f'cases/{experiment_case.name}/quasi_random_sample_robustness_upper_bound'] = \
                experiment_case.quasi_random_sample_robustness_upper_bound
        if experiment_case.quasi_random_sample_specification_satisfied is not None:
            hdf5_results[f'cases/{experiment_case.name}/quasi_random_sample_specification_satisfied'] = \
                experiment_case.quasi_random_sample_specification_satisfied
        # general information stored in the yaml file
        overview: typing.OrderedDict[Any, Any]
        if experiment_case.name in yaml_results['cases']:
            overview = OrderedDict(yaml_results['cases'][experiment_case.name])
        else:
            overview = OrderedDict()

        overview['repair_backend'] = str(experiment_case.repair_backend)
        if experiment_case.verifier is not None:
            overview['verifier'] = experiment_case.verifier
        if experiment_case.falsifiers is not None:
            overview['falsifiers'] = experiment_case.falsifiers
        if experiment_case.verifier_exit_mode is not None:
            overview['verifier_exit_mode'] = experiment_case.verifier_exit_mode
        if experiment_case.network_name is not None:
            overview['network_name'] = experiment_case.network_name
        overview['result'] = str(experiment_case.experiment_result)
        overview['not_verified_properties'] = experiment_case.not_verified_properties
        overview['initial_loss'] = experiment_case.initial_loss
        overview['final_loss'] = experiment_case.final_loss
        if experiment_case.final_training_set_accuracy is not None:
            overview['final_training_set_accuracy'] = experiment_case.final_training_set_accuracy
        if experiment_case.final_test_set_accuracy is not None:
            overview['final_test_set_accuracy'] = experiment_case.final_test_set_accuracy
        overview['repair_steps'] = experiment_case.repair_steps
        if len(experiment_case.number_of_training_iterations_of_run) > 0:
            training_iterations_per_run = \
                pandas.Series(experiment_case.number_of_training_iterations_of_run.values(), dtype='int')
            overview['training_iterations_per_run'] = discrete_summary(training_iterations_per_run)
        training_iterations_per_repair_step = []
        for rs in range(experiment_case.repair_steps):
            training_iterations_per_repair_step.append(sum(
                experiment_case.number_of_training_iterations_of_run[(rs_, bi)]
                for rs_, bi in experiment_case.number_of_training_iterations_of_run.keys()
                if rs_ == rs
            ))
        training_iterations_per_repair_step = pandas.Series(training_iterations_per_repair_step, dtype='int')
        overview['training_iterations_per_repair_step'] = discrete_summary(training_iterations_per_repair_step)
        if experiment_case.total_runtime is not None:
            overview['total_runtime'] = experiment_case.total_runtime
        if args.detailed_runtimes and experiment_case.runtimes is not None:
            overview['runtimes'] = experiment_case.runtimes
        if experiment_case.parameter_change_l1 is not None or experiment_case.parameter_change_l2 is not None \
                or experiment_case.parameter_change_linf is not None:
            overview['parameter_change'] = OrderedDict()
        if experiment_case.parameter_change_l1 is not None:
            overview['parameter_change']['l1'] = experiment_case.parameter_change_l1
        if experiment_case.parameter_change_l2 is not None:
            overview['parameter_change']['l2'] = experiment_case.parameter_change_l2
        if experiment_case.parameter_change_linf is not None:
            overview['parameter_change']['linf'] = experiment_case.parameter_change_linf
        if experiment_case.num_counterexamples is not None and \
                experiment_case.num_newly_introduced_counterexamples is not None:
            overview['counterexamples'] = OrderedDict([
                ('total', experiment_case.num_counterexamples),
                ('newly_introduced', experiment_case.num_newly_introduced_counterexamples),
            ])
        if experiment_case.counterexample_information is not None:
            if 'counterexamples' not in overview:
                overview['counterexamples'] = OrderedDict()
            overview['counterexamples']['information'] = experiment_case.counterexample_information
        # enforce a certain order in overview
        # this is only necessary if some groups were already present in the old results.yaml file
        # these would otherwise always come first
        if 'repair_backend' in overview:
            overview.move_to_end('repair_backend', last=True)
        if 'network_name' in overview:
            overview.move_to_end('network_name', last=True)
        if 'result' in overview:
            overview.move_to_end('result', last=True)
        if 'not_verified_properties' in overview:
            overview.move_to_end('not_verified_properties', last=True)
        if 'initial_loss' in overview:
            overview.move_to_end('initial_loss', last=True)
        if 'final_loss' in overview:
            overview.move_to_end('final_loss', last=True)
        if 'final_training_set_accuracy' in overview:
            overview.move_to_end('final_training_set_accuracy', last=True)
        if 'final_test_set_accuracy' in overview:
            overview.move_to_end('final_test_set_accuracy', last=True)
        if 'repair_steps' in overview:
            overview.move_to_end('repair_steps', last=True)
        if 'training_iterations_per_run' in overview:
            overview.move_to_end('training_iterations_per_run', last=True)
        if 'training_iterations_per_repair_step' in overview:
            overview.move_to_end('training_iterations_per_repair_step', last=True)
        if 'total_runtime' in overview:
            overview.move_to_end('total_runtime', last=True)
        if 'runtimes' in overview:
            overview.move_to_end('runtimes', last=True)
        if 'parameter_change' in overview:
            overview.move_to_end('parameter_change', last=True)
        if 'counterexamples' in overview:
            overview.move_to_end('counterexamples', last=True)
        yaml_results['cases'][experiment_case.name] = overview

    if full_experiment_runtime_summary is not None:
        hdf5_results['summary/runtimes'] = full_experiment_runtime_summary

    # aggregate results by certain experiment case groups
    # by default aggregate for all cases only
    case_groups = OrderedDict([('summary', experiment_cases)])
    if 'collision_detection_repair_4' in args.experiment_output_dir:
        # the collision detection repair 4 results need to be grouped by
        # the number of properties in the specification.
        collision_detection_single_property_case_rx = re.compile(r'(?P<i>\d+)$')
        collision_detection_multi_property_case_rx = re.compile(r'(?P<i>\d+)-(?P<j>\d+)$')
        case_groups['summary_1_data_point'] = []
        case_groups['summary_5_data_points'] = []
        case_groups['summary_10_data_points'] = []
        case_groups['summary_25_data_points'] = []
        # [case for case in experiment_cases if collision_detection_single_property_case_rx.match(case.name)]
        for experiment_case in experiment_cases:
            if collision_detection_single_property_case_rx.match(experiment_case.name):
                case_groups['summary_1_data_point'].append(experiment_case)
            match = collision_detection_multi_property_case_rx.match(experiment_case.name)
            if match is not None:
                num_data_points = int(match.group('j')) - int(match.group('i'))
                case_groups[f'summary_{num_data_points}_data_points'].append(experiment_case)

    print('Aggregating experiment case results')
    for group_name, case_group in case_groups.items():
        summary: typing.OrderedDict[Any, Any]
        if group_name in yaml_results:
            summary = OrderedDict(yaml_results[group_name])
        else:
            summary = OrderedDict()
        # summarise backend
        used_repair_backends = np.unique([str(case.repair_backend) for case in case_group])
        summary['repair_backends'] = used_repair_backends.tolist()
        # summarise verifiers
        used_verifiers = np.unique(
            [str(case.verifier) for case in case_group if case.verifier is not None])
        summary['verifiers'] = used_verifiers.tolist()
        # summarise falsifiers
        used_falsifiers = np.unique(
            [str(case.falsifiers) for case in case_group if case.falsifiers is not None])
        if len(used_falsifiers) > 0:
            summary['falsifiers'] = used_falsifiers.tolist()
        # summarise verifier exit modes
        used_verifier_exit_modes = np.unique(
            [str(case.verifier_exit_mode) for case in case_group if case.verifier_exit_mode is not None])
        if len(used_verifier_exit_modes) > 0:
            summary['verifier_exit_modes'] = used_verifier_exit_modes.tolist()
        # summarise networks
        used_network_names = np.unique(
            [case.network_name for case in case_group if case.network_name is not None])
        if len(used_network_names) > 0:
            summary['networks'] = used_network_names.tolist()
        # count successful, failing, aborted, etc, experiments
        summary['successful_experiments'] = OrderedDict()
        summary['successful_experiments']['cases'] = \
            [case.name for case in case_group if case.experiment_result == ExperimentResult.SUCCESS]
        summary['successful_experiments']['count'] = \
            sum(case.experiment_result == ExperimentResult.SUCCESS for case in case_group)
        summary['experiments_failing_due_to_backend_failure'] = OrderedDict()
        summary['experiments_failing_due_to_backend_failure']['cases'] = \
            [case.name for case in case_group if case.experiment_result == ExperimentResult.BACKEND_FAILURE]
        summary['experiments_failing_due_to_backend_failure']['count'] = \
            sum(case.experiment_result == ExperimentResult.BACKEND_FAILURE for case in case_group)
        summary['experiments_failing_due_to_max_iterations_exhausted'] = OrderedDict()
        summary['experiments_failing_due_to_max_iterations_exhausted']['cases'] = \
            [case.name for case in case_group
             if case.experiment_result == ExperimentResult.MAX_ITERATIONS_FAILURE]
        summary['experiments_failing_due_to_max_iterations_exhausted']['count'] = \
            sum(case.experiment_result == ExperimentResult.MAX_ITERATIONS_FAILURE for case in case_group)
        summary['experiments_failing_due_to_verification_problem'] = OrderedDict()
        summary['experiments_failing_due_to_verification_problem']['cases'] = \
            [case.name for case in case_group if case.experiment_result == ExperimentResult.VERIFICATION_PROBLEM]
        summary['experiments_failing_due_to_verification_problem']['count'] = \
            sum(case.experiment_result == ExperimentResult.VERIFICATION_PROBLEM for case in case_group)
        summary['experiments_failing_due_to_timeout'] = OrderedDict()
        summary['experiments_failing_due_to_timeout']['cases'] = \
            [case.name for case in case_group if case.experiment_result == ExperimentResult.TIMEOUT]
        summary['experiments_failing_due_to_timeout']['count'] = \
            sum(case.experiment_result == ExperimentResult.TIMEOUT for case in case_group)
        summary['aborted_experiments'] = OrderedDict()
        summary['aborted_experiments']['cases'] = \
            [case.name for case in case_group if case.experiment_result == ExperimentResult.ABORTED]
        summary['aborted_experiments']['count'] = \
            sum(case.experiment_result == ExperimentResult.ABORTED for case in case_group)
        # summarise not verified properties
        not_verified_properties = \
            pandas.Series(itertools.chain(*[case.not_verified_properties for case in case_group
                                            if case.not_verified_properties is not None]), dtype='str')
        summary['not_verified_property_counts'] = not_verified_properties.value_counts().to_dict()
        # summarise repair steps
        repair_steps = pandas.Series([case.repair_steps for case in case_group], dtype='int')
        summary['repair_steps'] = discrete_summary(repair_steps)
        # summarise training loop runs
        training_iterations_per_run = pandas.Series(
            itertools.chain(*[case.number_of_training_iterations_of_run.values() for case in case_group]),
            dtype=int
        )
        summary['training_iterations_per_run'] = discrete_summary(training_iterations_per_run)
        training_iterations_per_repair_step = []
        for case in case_group:
            for rs in range(case.repair_steps):
                training_iterations_per_repair_step.append(sum(
                    case.number_of_training_iterations_of_run[(rs_, bi)]
                    for rs_, bi in case.number_of_training_iterations_of_run.keys()
                    if rs_ == rs
                ))
        training_iterations_per_repair_step = pandas.Series(training_iterations_per_repair_step, dtype='int')
        summary['training_iterations_per_repair_step'] = \
            discrete_summary(training_iterations_per_repair_step)
        # summarise initial and final loss
        initial_losses = pandas.DataFrame([case.initial_loss for case in case_group if case.initial_loss is not None])
        # timed out experiments have no final loss, remove those
        final_losses = pandas.DataFrame([case.final_loss for case in case_group if case.final_loss is not None])
        if len(initial_losses) > 0:
            summary['initial_performance'] = data_frame_summary(initial_losses, 'continuous')
        if len(final_losses) > 0:
            summary['final_performance'] = data_frame_summary(final_losses, 'continuous')
        # summarise final loss for successful and verifier error cases only
        success_final_losses = pandas.DataFrame(
            [case.final_loss for case in case_group
             if case.final_loss is not None and
             case.experiment_result == ExperimentResult.SUCCESS],
            dtype='float64'
        )
        success_verification_problem_final_losses = pandas.DataFrame(
            [case.final_loss for case in case_group
             if case.final_loss is not None and
             case.experiment_result in (ExperimentResult.SUCCESS, ExperimentResult.VERIFICATION_PROBLEM)]
        )
        if len(success_final_losses) > 0:
            summary['success_final_performance'] = \
                data_frame_summary(success_final_losses, 'continuous')
        if len(success_verification_problem_final_losses) > 0:
            summary['success_or_verification_problem_final_performance'] = \
                data_frame_summary(success_verification_problem_final_losses, 'continuous')
        # do the same summaries for calculated final training set accuracies
        final_training_set_accuracies = pandas.DataFrame([case.final_training_set_accuracy for case in case_group
                                                          if case.final_training_set_accuracy is not None])
        if len(final_training_set_accuracies) > 0:
            summary['final_training_set_accuracy'] = data_frame_summary(final_training_set_accuracies, 'continuous')
        success_final_training_set_accuracies = pandas.DataFrame(
            [case.final_training_set_accuracy for case in case_group
             if case.final_training_set_accuracy is not None
             and case.experiment_result == ExperimentResult.SUCCESS],
            dtype='float64'
        )
        success_verification_problem_final_training_set_accuracies = pandas.DataFrame(
            [case.final_training_set_accuracy for case in case_group
             if case.final_training_set_accuracy is not None
             and case.experiment_result in (ExperimentResult.SUCCESS, ExperimentResult.VERIFICATION_PROBLEM)],
            dtype='float64'
        )
        if len(success_final_training_set_accuracies) > 0:
            summary['success_final_training_set_accuracy'] = \
                data_frame_summary(success_final_training_set_accuracies, 'continuous')
        if len(success_verification_problem_final_training_set_accuracies) > 0:
            summary['success_or_verification_problem_final_training_set_accuracy'] = \
                data_frame_summary(success_verification_problem_final_training_set_accuracies, 'continuous')
        # here we go with test set accuracies
        final_test_set_accuracies = pandas.DataFrame([case.final_test_set_accuracy for case in case_group
                                                      if case.final_test_set_accuracy is not None])
        if len(final_test_set_accuracies) > 0:
            summary['final_test_set_accuracy'] = data_frame_summary(final_test_set_accuracies, 'continuous')
        success_final_test_set_accuracies = pandas.DataFrame(
            [case.final_test_set_accuracy for case in case_group
             if case.final_test_set_accuracy is not None
             and case.experiment_result == ExperimentResult.SUCCESS],
            dtype='float64'
        )
        success_verification_problem_final_test_set_accuracies = pandas.DataFrame(
            [case.final_test_set_accuracy for case in case_group
             if case.final_test_set_accuracy is not None
             and case.experiment_result in (ExperimentResult.SUCCESS, ExperimentResult.VERIFICATION_PROBLEM)],
            dtype='float64'
        )
        if len(success_final_test_set_accuracies) > 0:
            summary['success_final_test_set_accuracy'] = \
                data_frame_summary(success_final_test_set_accuracies, 'continuous')
        if len(success_verification_problem_final_test_set_accuracies) > 0:
            summary['success_or_verification_problem_final_test_set_accuracy'] = \
                data_frame_summary(success_verification_problem_final_test_set_accuracies, 'continuous')

        total_runtimes = pandas.Series([
            case.total_runtime for case in case_group
            if case.total_runtime is not None
        ], dtype='float')
        if len(total_runtimes) > 0:
            summary['total_runtime']: continuous_summary(total_runtimes)

        # summarise num_counterexamples and num_newly_introduced_counterexamples
        num_counterexamples = pandas.Series([
            case.num_counterexamples for case in case_group
            if case.num_counterexamples is not None
        ], dtype='int')
        newly_introduced_counterexamples_fraction = pandas.Series([
            case.num_newly_introduced_counterexamples / case.num_counterexamples for case in case_group
            if case.num_counterexamples is not None
        ], dtype='float')
        if any(
                case.counterexample_violations_at_generation is not None
                and len(case.counterexample_violations_at_generation) > 0
                for case in case_group
        ):
            counterexample_violations_at_generation = pandas.Series(np.hstack([
                case.counterexample_violations_at_generation.to_numpy() for case in case_group
                if case.counterexample_violations_at_generation is not None
            ]), dtype='float64')
        else:
            counterexample_violations_at_generation = pandas.Series(dtype='float64')
        if len(num_counterexamples) > 0 or len(newly_introduced_counterexamples_fraction) > 0 \
                or len(counterexample_violations_at_generation) > 0:
            summary['counterexamples'] = OrderedDict()
        if len(num_counterexamples) > 0:
            summary['counterexamples']['amount_discovered'] = discrete_summary(num_counterexamples)
        if len(newly_introduced_counterexamples_fraction) > 0:
            summary['counterexamples']['fraction_newly_introduced'] = \
                continuous_summary(newly_introduced_counterexamples_fraction)
        if len(counterexample_violations_at_generation) > 0:
            summary['counterexamples']['violation_at_generation'] = \
                continuous_summary(counterexample_violations_at_generation)
        # summarise parameter changes
        l1_parameter_changes = pandas.Series([
            case.parameter_change_l1 for case in case_group
            if case.parameter_change_l1 is not None
        ], dtype='float64')
        l2_parameter_changes = pandas.Series([
            case.parameter_change_l2 for case in case_group
            if case.parameter_change_l2 is not None
        ], dtype='float64')
        linf_parameter_changes = pandas.Series([
            case.parameter_change_linf for case in case_group
            if case.parameter_change_linf is not None
        ], dtype='float64')
        if len(l1_parameter_changes) > 0 or len(l2_parameter_changes) > 0 or len(linf_parameter_changes) > 0:
            summary['parameter_change'] = OrderedDict()
        if len(l1_parameter_changes) > 0:
            summary['parameter_change']['l1'] = continuous_summary(l1_parameter_changes)
        if len(l2_parameter_changes) > 0:
            summary['parameter_change']['l2'] = continuous_summary(l2_parameter_changes)
        if len(linf_parameter_changes) > 0:
            summary['parameter_change']['linf'] = continuous_summary(linf_parameter_changes)

        # enforce a certain order in overview
        if 'repair_backends' in summary:
            summary.move_to_end('repair_backends', last=True)
        if 'networks' in summary:
            summary.move_to_end('networks', last=True)
        if 'successful_experiments' in summary:
            summary.move_to_end('successful_experiments', last=True)
        if 'experiments_failing_due_to_backend_failure' in summary:
            summary.move_to_end('experiments_failing_due_to_backend_failure', last=True)
        if 'experiments_failing_due_to_max_iterations_exhausted' in summary:
            summary.move_to_end('experiments_failing_due_to_max_iterations_exhausted', last=True)
        if 'experiments_failing_due_to_verification_problem' in summary:
            summary.move_to_end('experiments_failing_due_to_verification_problem', last=True)
        if 'experiments_failing_due_to_timeout' in summary:
            summary.move_to_end('experiments_failing_due_to_timeout', last=True)
        if 'aborted_experiments' in summary:
            summary.move_to_end('aborted_experiments', last=True)
        if 'not_verified_property_counts' in summary:
            summary.move_to_end('not_verified_property_counts', last=True)
        if 'repair_steps' in summary:
            summary.move_to_end('repair_steps', last=True)
        if 'training_iterations_per_run' in summary:
            summary.move_to_end('training_iterations_per_run', last=True)
        if 'training_iterations_per_repair_step' in summary:
            summary.move_to_end('training_iterations_per_repair_step', last=True)
        if 'initial_performance' in summary:
            summary.move_to_end('initial_performance', last=True)
        if 'final_performance' in summary:
            summary.move_to_end('final_performance', last=True)
        if 'success_final_performance' in summary:
            summary.move_to_end('success_final_performance', last=True)
        if 'success_or_verification_problem_final_performance' in summary:
            summary.move_to_end('success_or_verification_problem_final_performance', last=True)
        if 'final_training_set_accuracy' in summary:
            summary.move_to_end('final_training_set_accuracy', last=True)
        if 'success_final_training_set_accuracy' in summary:
            summary.move_to_end('success_final_training_set_accuracy', last=True)
        if 'success_or_verification_problem_final_training_set_accuracy' in summary:
            summary.move_to_end('success_or_verification_problem_final_training_set_accuracy', last=True)
        if 'final_test_set_accuracy' in summary:
            summary.move_to_end('final_test_set_accuracy', last=True)
        if 'success_final_test_set_accuracy' in summary:
            summary.move_to_end('success_final_test_set_accuracy', last=True)
        if 'success_or_verification_problem_final_test_set_accuracy' in summary:
            summary.move_to_end('success_or_verification_problem_final_test_set_accuracy', last=True)
        if 'total_runtime' in summary:
            summary.move_to_end('total_runtime', last=True)
        if 'counterexamples' in summary:
            summary.move_to_end('counterexamples', last=True)
        if 'parameter_change' in summary:
            summary.move_to_end('parameter_change', last=True)
        yaml_results[group_name] = summary

    yaml_results.move_to_end('cases', last=True)

    with open(Path(output_dir, 'results.yaml'), 'w+t') as yaml_results_file:
        yaml.dump(yaml_results, yaml_results_file)
    with pandas.HDFStore(str(Path(output_dir, 'results.h5')), mode='a') as hdf5_results_store:
        for key, value in hdf5_results.items():
            hdf5_results_store.put(key, value, format='table')
